# 合并过滤器

**splash 合并redis 的 dupefilter** 

```python
from scrapy_redis.dupefilter import RFPDupeFilter as BaseRFPDupeFilter
from __future__ import absolute_import
from copy import deepcopy
from scrapy.dupefilters import RFPDupeFilter
from scrapy.utils.url import canonicalize_url
from scrapy.utils.request import request_fingerprint
from scrapy_splash.utils import dict_hash

# 下面这个从scrapy_splash 赋值 出来
def splash_request_fingerprint(request, include_headers=None)

# 此时已经合并好了
class MyDupeFilter(BaseRFPDupeFilter):

    def request_fingerprint(self, request):
        return splash_request_fingerprint(request)
```



**布隆, splash过滤器 , redis过滤器的合并**

```python
from __future__ import absolute_import
from copy import deepcopy
from scrapy.dupefilters import RFPDupeFilter
from scrapy.utils.url import canonicalize_url
from scrapy.utils.request import request_fingerprint
from scrapy_splash.utils import dict_hash

import logging
import time
from scrapy_redis_bloomfilter.defaults import BLOOMFILTER_HASH_NUMBER, BLOOMFILTER_BIT, DUPEFILTER_DEBUG
from scrapy_redis_bloomfilter import defaults
from scrapy_redis.connection import get_redis_from_settings
from scrapy_redis_bloomfilter.bloomfilter import BloomFilter
from scrapy_redis.dupefilter import RFPDupeFilter as BaseDupeFilter

logger = logging.getLogger(__name__)

def splash_request_fingerprint(request, include_headers=None):
    fp = request_fingerprint(request, include_headers=include_headers)
    if 'splash' not in request.meta:
        return fp

    splash_options = deepcopy(request.meta['splash'])
    args = splash_options.setdefault('args', {})

    if 'url' in args:
        args['url'] = canonicalize_url(args['url'], keep_fragments=True)

    return dict_hash(splash_options, fp)

class MyDupeFilter(BaseDupeFilter):
    logger = logger

    def __init__(self, server, key, debug, bit, hash_number):
        self.server = server
        self.key = key
        self.debug = debug
        self.bit = bit
        self.hash_number = hash_number
        self.logdupes = True
        self.bf = BloomFilter(server, self.key, bit, hash_number)

    @classmethod
    def from_settings(cls, settings):
       server = get_redis_from_settings(settings)
        key = defaults.DUPEFILTER_KEY % {'timestamp': int(time.time())}
        debug = settings.getbool('DUPEFILTER_DEBUG', DUPEFILTER_DEBUG)
        bit = settings.getint('BLOOMFILTER_BIT', BLOOMFILTER_BIT)
        hash_number = settings.getint('BLOOMFILTER_HASH_NUMBER', BLOOMFILTER_HASH_NUMBER)
        return cls(server, key=key, debug=debug, bit=bit, hash_number=hash_number)

    @classmethod
    def from_crawler(cls, crawler):
       instance = cls.from_settings(crawler.settings)
        return instance

    @classmethod
    def from_spider(cls, spider):
       settings = spider.settings
        server = get_redis_from_settings(settings)
        dupefilter_key = settings.get("SCHEDULER_DUPEFILTER_KEY", defaults.SCHEDULER_DUPEFILTER_KEY)
        key = dupefilter_key % {'spider': spider.name}
        debug = settings.getbool('DUPEFILTER_DEBUG', DUPEFILTER_DEBUG)
        bit = settings.getint('BLOOMFILTER_BIT', BLOOMFILTER_BIT)
        hash_number = settings.getint('BLOOMFILTER_HASH_NUMBER', BLOOMFILTER_HASH_NUMBER)
        print(key, bit, hash_number)
        instance = cls(server, key=key, debug=debug, bit=bit, hash_number=hash_number)
        return instance

    def request_seen(self, request):
       fp = self.request_fingerprint(request)
        # This returns the number of values added, zero if already exists.
        if self.bf.exists(fp):
            return True
        self.bf.insert(fp)
        return False

    def log(self, request, spider):
        if self.debug:
            msg = "Filtered duplicate request: %(request)s"
            self.logger.debug(msg, {'request': request}, extra={'spider': spider})
        elif self.logdupes:
            msg = ("Filtered duplicate request %(request)s"
                   " - no more duplicates will be shown"
                   " (see DUPEFILTER_DEBUG to show all duplicates)")
            self.logger.debug(msg, {'request': request}, extra={'spider': spider})
            self.logdupes = False
        spider.crawler.stats.inc_value('bloomfilter/filtered', spider=spider)


    def request_fingerprint(self, request):
        return splash_request_fingerprint(request)
```







# 实战抓取工程题库

中大网校



创建entry 文件

```python
from scrapy.cmdline import execute

if __name__ == "__main__":
    
    execute("scrapy crawl kaoshi".split()) # 执行, 可以运行debug
```



```python
class KaoshiSpider(Reids.spider):
    
    name = "kaoshi"
    allowed_domains = ["wangxiao.cn"]
    start_urls = ["http://ks.wangxiao.cn"]
    
    def parse(self, resp, **kwargs):
        
        # 拿到所有类目的页面
        le = LinkExtractor(restrict_xpath=("//ul",))
        a_list = le.extract_links(resp)
        
        
        for a in a_list:
            
            first_title = a.text
            exampoint_url = a.url.replace("TestPage", "exampoint")
            
            # 请求类目页面
            yield scrapy.Request(
            	url = exampoint_url,
                callback = self.parse_second_level
                meta={"first_title": first_title}
            )
            break
        
    def parse_second_level(self, resp, **kwargs):
        
        first_title = resp.meta["first"]
        
        #提取子类目
        le = LinkExtractor(restrict_xpath=("//ul",))
        a_list = le.extract_links(resp)
        
        # 分别请求子类目
        for a in a_list:
            second_title = a.text
            
            yield scrapy.Request(
            	url=a.url,
                callback=self.parse_third_level,
                meta={"first_title": first_title, "second_title": second_title}
            )
            break
        
    def parse_third_level(self, point, **kwargs):
        first_title = resp.meta["first_title"]
        second_title = resp.meta["second_title"]
        
        # 拿到所有列表
        points = point.xpath("//ul[@class='section-point-item']")
        
        for point in points:
            # 遍历列表 从当前节点向上找跟祖父节点和父节点
            parents = point.xpath("./ancestor-or-self::ul[@class='chapter-item' or @class='section-item']")
        	p_list = [first_title, second_title]
        	for p in parents: 
                fu_name = "".join(point.xpath("./li[1]/text()").extract().strip().repalce(" ", ""))
    			p_list.append(fu_name)
                # 所有层级分类
            print(p_list)
            
       	# 拿到当前关注的的文件名
       	point_name = "".join(point.xpath("./li[1]/text()").extract().strip().repalce(" ", ""))
       	
        
    	# post 请求数据
        post_dic = {
            
        }
    	
        # 请求拿到题目
        yield scrapy.Request(
        	url="",
            method="post",
            headers={
                x-http,
                content-type
            },
            body=json.dupms(post_dic),
            callback=self.parse_detail,
            meta={
                "p_list": p_list,
                "p_name": point_name
            }
        )
        
    
    def parse_detail(self, resp, **kwargs):
        # 获取内容, 获取题目 和答案        
        
        p_list = resp.meta["p_list"]
        p_name = resp.meta["p_name"]
        
        
     	# 请求内容
    	all_data = resp.json()
        data = all_data.get("Data")
    	
        all_questsions = []
        
        # 这个循环拿到所有的题目
        for ds in data: 
            
            questions = ds.get("questions")
            
            if questions:
                
                #for q in questions:
                    
                 #   all_questsions.append(q)
                
                all_questsions.extend(questions)
           	
            else:
                
                materials = ds.get("materials")
            	
                
                for m in materials:
                    
                    qs = m.get("questsions")
                    
                    all_questsions.extend(qs)
            
        # 处理拿到题目和答案的文本
        for q in all_questsions:
            content = q.get("content")
            text_analysis = q.get("textAnalysis")
            
            opts = q.get("options")
            options = []
            right_options = []
            
            # 每一个选项
            for opt in opts:
                
                name = opt.get("name")
                content = opt.get("content")
                xuanxiang = f{name}.{content}
                options.append(xuanxiang)
                is_right = opt.get("isRight")
                
                if is_right:
                    
                    right_options.append(name)
            # 拼接题目整体
  			content +=  """
  			
  				<p>
  					{"<br/>".join(options)}	
  				</p>
  			
  			""".replace(" ", "").replace("\n","")
            
            
            # 拼接答案整体
            text_andysis = """
            	<p>
  					{",".join(right_options)}	
  				</p>
            
            """.replace(" ", "").replace("\n","") + text_analysis
            
            
            # QuestsionsItem
            
           	item  = QuestsionsItem()
           
            item["content"] = content
            item["text_andysis"] = text_andysis
            item["p_list"] = p_list
            item["p_name"] = p_name
            
    
```



pipeline

```
def xiaoPipeline:

	def process_item(self, item):
		
		content = item["content"]
		text_analysis = item["text_analysis"]
		p_list = item["p_list"]
		p_name = item["p_name"]
		
		paths = os.path.join(*p_list)
		
		if not os.path.exists(paths):
			os.makedirs(paths)
		
		full_md_file_path = os.path.join(paths, f"{p_name}.md")
		
		with open(full_md_file_path, mode="a", encoding="utf-8") as f:
		
			f.write(content)
			f.write("\n")
			f.write(text_analysis)
			f.write("\n")
			f.write("------")
			f.write("\n")
		
		return item 





# 先下载图片
def imagePipeline(ImagePipeline):

	def get_media_request(self, item, info):
		content = item["content"]
		text_analysis = item["text_analysis"]
		ct = etree.HTML(content)
		ta = etree.HTML(text_analysis)
		
		srcs = ct.xpath("//img/@href")
		srcs.extend(ta.xpath("//img/@href"))
		
		for src in srcs:
		
			yield scrapy.Request(
				url=src
			)
	def file_path(self, request, response=None, info=None):
		file_name = request.url.split("/")[-1]
		p_list = request.meta["p_list"]
		p_name = request.meta["p_name"]
		img_path = os.path.join(*p_list, f"{p_name}_img")
		return os.path.join(img_path, file_name)
	
    def item_completed(self, result, item, info):
    	
    	for status, r in results:
    	
    		if status:
    			img_url = r.get("url")
    			img_path = r.get("path")
    			
    			img_my_path = os.path.join(*img_path.split("/")[-2:])
    			
    			item["content"] = item["content"].replace(img_url, img_my_path) 
    			item["text_analysis"] = item["text_analysis"].replace(img_url, img_my_path) 
    	
    
    	return item
		
```





## 报错问题: 

```
xpath 没有 @class='section-point-item', 而是直接sections-item

字符串没有 replace

```







# 实战二: 中大登录逆向分析